<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Bingliang Zhang</title>
  
  <meta name="author" content="Bingliang Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Bingliang Zhang</name>
              </p>
              <p>I am an undergraduate student in <a href="https://iiis.tsinghua.edu.cn/en/yaoclass/">Yao Class</a> (a special computer science pivot class found by <a href="https://iiis.tsinghua.edu.cn/yao/">Andrew Chi-Chih Yao</a>) at <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>.
              </p>
              <p>
                My major is <b>Computer Science and Technology</b>. I am applying for cs Ph.D. programs in 2023 fall and my research interests lie in computer vision specifically image synthesis, neural 3D shape representations and few-shot learning.
              </p>
              <p style="text-align:center">
                <a href="mailto:bingliangzhang00@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/CV_BingliangZhang.pdf">CV</a> &nbsp/&nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:40%">
              <a href="images/Bingliang.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Bingliang.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr>
            <td style="padding:20px;width:35%;vertical-align:bottom">
              <div class="one">
                <img src='images/custom-finetuning.png' width="250">
              </div>
            </td>
            <td style="padding:20px;width:65%;vertical-align:top">
              <a href="https://arxiv.org/abs/2212.04488">
                <papertitle>Multi-concept Customization of Text-to-Image Diffusion</papertitle>
              </a>
              <br>
              <a href="https://nupurkmr9.github.io/">Nupur Kumari</a>, 
              Bingliang Zhang, 
              <a href="https://richzhang.github.io/">Richard Zhang</a>, 
              <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, 
              <a href="https://www.cs.cmu.edu/~junyanz/">Junyan Zhu</a>, 
              <p></p>
              <p>
                An efficient method for augmenting existing text-to-image models to learn new concepts with few examples without forgeting learned knowledge. Also we propose a close-form optimization strategy for combining learned weights of multiple concepts.
              </p>
            </td>
          </tr>		

          <tr>
            <td style="padding:20px;width:35%;vertical-align:bottom">
              <div class="one">
                <img src='images/rspo.png' width="250">
              </div>
            </td>
            <td style="padding:20px;width:65%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2204.02246">
                <papertitle>Continuously Discovering Novel Strategies via Reward-Switching Policy Optimization</papertitle>
              </a>
              <br>
              <a href="https://zihan.page/">Zihan Zhou</a>, 
              <a href="https://openreview.net/profile?id=~Wei_Fu1/">Wei Fu</a>, 
              Bingliang Zhang, 
              <a href="https://jxwuyi.weebly.com/">Yi Wu</a>, 
              <br>
              <em>ICLR</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2204.02246">arXiv</a>
              <p></p>
              <p>
                A paradigm to discover diverse strategies in complex RL environments. Our method is able to discover a wide spectrum of strategies in a variety of domains.
              </p>
            </td>
          </tr>		

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Selected Course Project</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:35%;vertical-align:bottom">
              <div class="one">
                <img src='images/protein.png' width="250">
              </div>
            </td>
            <td style="padding:20px;width:65%;vertical-align:top">
              <a href="data/protein.pdf">
                <papertitle>Cryo2SSNet: 2-Stage Search Supervised Network for Cryo-electron Microscopy Single-Particle Reconstruction</papertitle>
              </a>
              <br>
              <em>Tsinghua University</em>, Fall 2021
              <br>
              <p>
                We propose a two-stage Search Supervised Network (Cryo2SSNet), an auto-encoder architecture to reconstruct a 3D density map in a two-stage pipeline. Our method can achieve a high reconstruction accuracy and efficiency.
              </p>
            </td>
          </tr>		
          
          <tr>
            <td style="padding:20px;width:35%;vertical-align:bottom">
              <div class="one">
                <img src='images/odqa.png' width="250">
              </div>
            </td>
            <td style="padding:20px;width:65%;vertical-align:top">
              <a href="data/odqa.pdf">
                <papertitle>Open Domain Question Answering with Generative Augmentation</papertitle>
              </a>
              <br>
              <em>Tsinghua University</em>, Fall 2021
              <br>
              <p>
                Improved the previous Dense Passage Retriever with Generation-Augmented Retrieval and propose a modified contrastive loss. Our proposed method can improve around 13.1% on top 100 retrieval accuracy.
              </p>
            </td>
          </tr>		

          <tr>
            <td style="padding:20px;width:35%;vertical-align:bottom">
              <div class="one">
                <img src='images/ai-gomuko.png' width="250">
              </div>
            </td>
            <td style="padding:20px;width:65%;vertical-align:middle">
              <a href="data/ai-gomuko.pdf">
                <papertitle>Intelligent Gomuko: Reinforcement Learning with Advisor</papertitle>
              </a>
              <br>
              <em>Tsinghua University</em>, Fall 2020
              <br>
              <p>
                A simplified version of ‚ÄúAlphaGo‚Äù on Gomuko, using min-max search data as supervision.   It can achieve intelligent human-machine competition and adjustable difficulty.
              </p>
            </td>
          </tr>		

					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Website credits: <a href="https://github.com/jonbarron/website">Jon Barron</a>. Thanks to him!
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
